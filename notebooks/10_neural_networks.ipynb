{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Neural Networks（ニューラルネットワーク）\n",
    "\n",
    "このノートブックは [Nature of Code](https://natureofcode.com/neural-networks/) の第10章をJuliaで実装したものです。\n",
    "\n",
    "## 概要\n",
    "\n",
    "ニューラルネットワークは、生物の神経系を模倣した計算モデルです。\n",
    "入力から出力への非線形なマッピングを学習することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 パーセプトロン（Perceptron）\n",
    "\n",
    "最も単純なニューラルネットワークの構成単位。\n",
    "複数の入力を受け取り、重み付け和を計算し、活性化関数を通して出力を生成します。\n",
    "\n",
    "### パーセプトロンの3ステップ\n",
    "\n",
    "1. **重み付け**: 各入力に重みを掛ける\n",
    "2. **加算**: 重み付けされた入力を合計\n",
    "3. **活性化**: 合計を活性化関数に通す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Perceptron\n",
    "    weights::Vector{Float64}\n",
    "    learningRate::Float64\n",
    "end\n",
    "\n",
    "function Perceptron(n::Int; learningRate=0.01)\n",
    "    weights = [rand() * 2 - 1 for _ in 1:n]  # -1 ~ 1\n",
    "    Perceptron(weights, learningRate)\n",
    "end\n",
    "\n",
    "# 活性化関数（ステップ関数）\n",
    "function activate(sum::Float64)\n",
    "    return sum >= 0 ? 1 : -1\n",
    "end\n",
    "\n",
    "# 順伝播\n",
    "function feedforward(p::Perceptron, inputs::Vector{Float64})\n",
    "    # 重み付け和を計算\n",
    "    sum = dot(inputs, p.weights)\n",
    "    # 活性化\n",
    "    return activate(sum)\n",
    "end\n",
    "\n",
    "# 訓練（1サンプル）\n",
    "function train!(p::Perceptron, inputs::Vector{Float64}, target::Int)\n",
    "    guess = feedforward(p, inputs)\n",
    "    error = target - guess\n",
    "    \n",
    "    # 重みを更新\n",
    "    # new_weight = weight + error * input * learning_rate\n",
    "    for i in eachindex(p.weights)\n",
    "        p.weights[i] += error * inputs[i] * p.learningRate\n",
    "    end\n",
    "    \n",
    "    return error\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例：線形分類問題\n",
    "# 目標：y = 0.5x + 1 の上下を分類\n",
    "\n",
    "function f(x)\n",
    "    return 0.5 * x + 1\n",
    "end\n",
    "\n",
    "# 訓練データを生成\n",
    "function generateTrainingData(n::Int)\n",
    "    data = []\n",
    "    for _ in 1:n\n",
    "        x = rand() * 200 - 100  # -100 ~ 100\n",
    "        y = rand() * 200 - 100\n",
    "        \n",
    "        # 線の上なら1、下なら-1\n",
    "        label = y > f(x) ? 1 : -1\n",
    "        \n",
    "        # バイアス項を追加（常に1）\n",
    "        push!(data, ([x, y, 1.0], label))\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "# 訓練\n",
    "p = Perceptron(3, learningRate=0.001)\n",
    "trainingData = generateTrainingData(2000)\n",
    "\n",
    "errors = Float64[]\n",
    "for (inputs, target) in trainingData\n",
    "    err = train!(p, inputs, target)\n",
    "end\n",
    "\n",
    "println(\"学習後の重み: $(p.weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を可視化\n",
    "function visualize_perceptron(p::Perceptron)\n",
    "    # テストデータ\n",
    "    testData = generateTrainingData(200)\n",
    "    \n",
    "    # 正解と予測を分類\n",
    "    correct_above = Tuple{Float64, Float64}[]\n",
    "    correct_below = Tuple{Float64, Float64}[]\n",
    "    wrong = Tuple{Float64, Float64}[]\n",
    "    \n",
    "    for (inputs, target) in testData\n",
    "        guess = feedforward(p, inputs)\n",
    "        if guess == target\n",
    "            if target == 1\n",
    "                push!(correct_above, (inputs[1], inputs[2]))\n",
    "            else\n",
    "                push!(correct_below, (inputs[1], inputs[2]))\n",
    "            end\n",
    "        else\n",
    "            push!(wrong, (inputs[1], inputs[2]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # プロット\n",
    "    plt = plot(xlims=(-100, 100), ylims=(-100, 100), aspect_ratio=:equal)\n",
    "    \n",
    "    # 真の境界線\n",
    "    xs = -100:100\n",
    "    plot!(plt, xs, f.(xs), label=\"True boundary\", linewidth=2, color=:black)\n",
    "    \n",
    "    # 学習した境界線 (w1*x + w2*y + w3 = 0 → y = -(w1*x + w3)/w2)\n",
    "    learned_y(x) = -(p.weights[1] * x + p.weights[3]) / p.weights[2]\n",
    "    plot!(plt, xs, learned_y.(xs), label=\"Learned boundary\", \n",
    "          linewidth=2, color=:red, linestyle=:dash)\n",
    "    \n",
    "    # データ点\n",
    "    if !isempty(correct_above)\n",
    "        scatter!(plt, [c[1] for c in correct_above], [c[2] for c in correct_above],\n",
    "                 color=:blue, markersize=4, label=\"Correct (above)\")\n",
    "    end\n",
    "    if !isempty(correct_below)\n",
    "        scatter!(plt, [c[1] for c in correct_below], [c[2] for c in correct_below],\n",
    "                 color=:green, markersize=4, label=\"Correct (below)\")\n",
    "    end\n",
    "    if !isempty(wrong)\n",
    "        scatter!(plt, [w[1] for w in wrong], [w[2] for w in wrong],\n",
    "                 color=:red, markersize=6, markershape=:x, label=\"Wrong\")\n",
    "    end\n",
    "    \n",
    "    accuracy = (length(correct_above) + length(correct_below)) / length(testData) * 100\n",
    "    title!(\"Perceptron Classification (Accuracy: $(round(accuracy, digits=1))%)\")\n",
    "    \n",
    "    return plt\n",
    "end\n",
    "\n",
    "visualize_perceptron(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 多層パーセプトロン（MLP）\n",
    "\n",
    "単一のパーセプトロンは線形分離可能な問題しか解けません（XOR問題は解けない）。\n",
    "隠れ層を追加することで、非線形な問題も解けるようになります。\n",
    "\n",
    "### ネットワーク構造\n",
    "\n",
    "```\n",
    "入力層 → 隠れ層 → 出力層\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数（シグモイド）\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "sigmoid_derivative(x) = x * (1 - x)\n",
    "\n",
    "# 多層ニューラルネットワーク\n",
    "mutable struct NeuralNetwork\n",
    "    inputNodes::Int\n",
    "    hiddenNodes::Int\n",
    "    outputNodes::Int\n",
    "    weightsIH::Matrix{Float64}  # 入力→隠れ層\n",
    "    weightsHO::Matrix{Float64}  # 隠れ層→出力\n",
    "    biasH::Vector{Float64}\n",
    "    biasO::Vector{Float64}\n",
    "    learningRate::Float64\n",
    "end\n",
    "\n",
    "function NeuralNetwork(inputNodes::Int, hiddenNodes::Int, outputNodes::Int; \n",
    "                       learningRate=0.1)\n",
    "    # 重みをランダム初期化（Xavier初期化）\n",
    "    weightsIH = randn(hiddenNodes, inputNodes) * sqrt(2.0 / inputNodes)\n",
    "    weightsHO = randn(outputNodes, hiddenNodes) * sqrt(2.0 / hiddenNodes)\n",
    "    biasH = zeros(hiddenNodes)\n",
    "    biasO = zeros(outputNodes)\n",
    "    \n",
    "    NeuralNetwork(inputNodes, hiddenNodes, outputNodes, \n",
    "                  weightsIH, weightsHO, biasH, biasO, learningRate)\n",
    "end\n",
    "\n",
    "# 順伝播\n",
    "function feedforward(nn::NeuralNetwork, inputs::Vector{Float64})\n",
    "    # 隠れ層\n",
    "    hidden = sigmoid.(nn.weightsIH * inputs .+ nn.biasH)\n",
    "    \n",
    "    # 出力層\n",
    "    output = sigmoid.(nn.weightsHO * hidden .+ nn.biasO)\n",
    "    \n",
    "    return output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 バックプロパゲーション（誤差逆伝播法）\n",
    "\n",
    "多層ネットワークを訓練するアルゴリズム。\n",
    "出力層の誤差を、各層の重みの寄与度に応じて逆方向に伝播させます。\n",
    "\n",
    "### アルゴリズム\n",
    "\n",
    "1. 順伝播で出力を計算\n",
    "2. 出力誤差を計算\n",
    "3. 誤差を逆伝播して各層の重みを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(nn::NeuralNetwork, inputs::Vector{Float64}, targets::Vector{Float64})\n",
    "    # === 順伝播 ===\n",
    "    # 隠れ層\n",
    "    hiddenInputs = nn.weightsIH * inputs .+ nn.biasH\n",
    "    hiddenOutputs = sigmoid.(hiddenInputs)\n",
    "    \n",
    "    # 出力層\n",
    "    outputInputs = nn.weightsHO * hiddenOutputs .+ nn.biasO\n",
    "    outputs = sigmoid.(outputInputs)\n",
    "    \n",
    "    # === 誤差計算 ===\n",
    "    outputErrors = targets .- outputs\n",
    "    \n",
    "    # 隠れ層の誤差（出力層の誤差を逆伝播）\n",
    "    hiddenErrors = nn.weightsHO' * outputErrors\n",
    "    \n",
    "    # === 重み更新 ===\n",
    "    # 出力層の勾配\n",
    "    outputGradients = outputErrors .* sigmoid_derivative.(outputs) * nn.learningRate\n",
    "    \n",
    "    # 隠れ層→出力層の重み更新\n",
    "    nn.weightsHO .+= outputGradients * hiddenOutputs'\n",
    "    nn.biasO .+= outputGradients\n",
    "    \n",
    "    # 隠れ層の勾配\n",
    "    hiddenGradients = hiddenErrors .* sigmoid_derivative.(hiddenOutputs) * nn.learningRate\n",
    "    \n",
    "    # 入力層→隠れ層の重み更新\n",
    "    nn.weightsIH .+= hiddenGradients * inputs'\n",
    "    nn.biasH .+= hiddenGradients\n",
    "    \n",
    "    return sum(outputErrors.^2)  # 二乗誤差を返す\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR問題を学習\n",
    "function train_xor()\n",
    "    nn = NeuralNetwork(2, 4, 1, learningRate=0.5)\n",
    "    \n",
    "    # XORの訓練データ\n",
    "    trainingData = [\n",
    "        ([0.0, 0.0], [0.0]),\n",
    "        ([0.0, 1.0], [1.0]),\n",
    "        ([1.0, 0.0], [1.0]),\n",
    "        ([1.0, 1.0], [0.0])\n",
    "    ]\n",
    "    \n",
    "    errors = Float64[]\n",
    "    \n",
    "    for epoch in 1:10000\n",
    "        totalError = 0.0\n",
    "        for (inputs, targets) in trainingData\n",
    "            err = train!(nn, inputs, targets)\n",
    "            totalError += err\n",
    "        end\n",
    "        push!(errors, totalError / 4)\n",
    "        \n",
    "        if epoch % 2000 == 0\n",
    "            println(\"Epoch $epoch: Average Error = $(round(totalError/4, digits=6))\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # テスト\n",
    "    println(\"\\nTest Results:\")\n",
    "    for (inputs, targets) in trainingData\n",
    "        output = feedforward(nn, inputs)\n",
    "        println(\"$(inputs) -> $(round(output[1], digits=3)) (expected: $(targets[1]))\")\n",
    "    end\n",
    "    \n",
    "    return nn, errors\n",
    "end\n",
    "\n",
    "nn_xor, xor_errors = train_xor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線をプロット\n",
    "plot(xor_errors[1:100:end], xlabel=\"Epoch (×100)\", ylabel=\"Average Error\",\n",
    "     title=\"XOR Learning Curve\", legend=false, linewidth=2, yscale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XORの決定境界を可視化\n",
    "function visualize_xor(nn::NeuralNetwork)\n",
    "    xs = range(0, 1, length=50)\n",
    "    ys = range(0, 1, length=50)\n",
    "    \n",
    "    Z = zeros(length(xs), length(ys))\n",
    "    \n",
    "    for (i, x) in enumerate(xs)\n",
    "        for (j, y) in enumerate(ys)\n",
    "            Z[j, i] = feedforward(nn, [x, y])[1]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    heatmap(xs, ys, Z, c=:RdBu, clims=(0, 1),\n",
    "            xlabel=\"x₁\", ylabel=\"x₂\", title=\"XOR Decision Boundary\")\n",
    "    \n",
    "    # データ点を追加\n",
    "    scatter!([0, 1], [0, 1], color=:blue, markersize=10, label=\"Class 0\")\n",
    "    scatter!([0, 1], [1, 0], color=:red, markersize=10, label=\"Class 1\")\n",
    "end\n",
    "\n",
    "visualize_xor(nn_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 ニューラルネットワークの構成要素\n",
    "\n",
    "### 重み（Weights）\n",
    "ニューロン間の接続強度。学習によって調整される。\n",
    "\n",
    "### バイアス（Bias）\n",
    "各ニューロンのしきい値を調整。常に1の入力に掛けられる重み。\n",
    "\n",
    "### 活性化関数（Activation Function）\n",
    "非線形性を導入する関数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 様々な活性化関数\n",
    "\n",
    "# シグモイド\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "# tanh\n",
    "tanh_act(x) = tanh(x)\n",
    "\n",
    "# ReLU\n",
    "relu(x) = max(0, x)\n",
    "\n",
    "# Leaky ReLU\n",
    "leaky_relu(x) = x > 0 ? x : 0.01 * x\n",
    "\n",
    "# 可視化\n",
    "x = range(-5, 5, length=100)\n",
    "\n",
    "p1 = plot(x, sigmoid.(x), title=\"Sigmoid\", legend=false, linewidth=2)\n",
    "p2 = plot(x, tanh_act.(x), title=\"tanh\", legend=false, linewidth=2)\n",
    "p3 = plot(x, relu.(x), title=\"ReLU\", legend=false, linewidth=2)\n",
    "p4 = plot(x, leaky_relu.(x), title=\"Leaky ReLU\", legend=false, linewidth=2)\n",
    "\n",
    "plot(p1, p2, p3, p4, layout=(2, 2), size=(600, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Neuroevolution（神経進化）\n",
    "\n",
    "遺伝的アルゴリズムでニューラルネットワークの重みを進化させます。\n",
    "バックプロパゲーションの代わりに、自然選択で学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークのDNA（重みの配列）\n",
    "mutable struct NeuroBrain\n",
    "    nn::NeuralNetwork\n",
    "    fitness::Float64\n",
    "end\n",
    "\n",
    "function NeuroBrain(inputNodes::Int, hiddenNodes::Int, outputNodes::Int)\n",
    "    nn = NeuralNetwork(inputNodes, hiddenNodes, outputNodes)\n",
    "    NeuroBrain(nn, 0.0)\n",
    "end\n",
    "\n",
    "# DNAを配列として取得\n",
    "function getDNA(brain::NeuroBrain)\n",
    "    return vcat(vec(brain.nn.weightsIH), vec(brain.nn.weightsHO), \n",
    "                brain.nn.biasH, brain.nn.biasO)\n",
    "end\n",
    "\n",
    "# 配列からDNAを設定\n",
    "function setDNA!(brain::NeuroBrain, dna::Vector{Float64})\n",
    "    idx = 1\n",
    "    \n",
    "    # weightsIH\n",
    "    for i in eachindex(brain.nn.weightsIH)\n",
    "        brain.nn.weightsIH[i] = dna[idx]\n",
    "        idx += 1\n",
    "    end\n",
    "    \n",
    "    # weightsHO\n",
    "    for i in eachindex(brain.nn.weightsHO)\n",
    "        brain.nn.weightsHO[i] = dna[idx]\n",
    "        idx += 1\n",
    "    end\n",
    "    \n",
    "    # biasH\n",
    "    for i in eachindex(brain.nn.biasH)\n",
    "        brain.nn.biasH[i] = dna[idx]\n",
    "        idx += 1\n",
    "    end\n",
    "    \n",
    "    # biasO\n",
    "    for i in eachindex(brain.nn.biasO)\n",
    "        brain.nn.biasO[i] = dna[idx]\n",
    "        idx += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "# 交叉\n",
    "function crossover(a::NeuroBrain, b::NeuroBrain)\n",
    "    child = NeuroBrain(a.nn.inputNodes, a.nn.hiddenNodes, a.nn.outputNodes)\n",
    "    \n",
    "    dnaA = getDNA(a)\n",
    "    dnaB = getDNA(b)\n",
    "    childDNA = similar(dnaA)\n",
    "    \n",
    "    midpoint = rand(1:length(dnaA))\n",
    "    for i in eachindex(childDNA)\n",
    "        if i < midpoint\n",
    "            childDNA[i] = dnaA[i]\n",
    "        else\n",
    "            childDNA[i] = dnaB[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    setDNA!(child, childDNA)\n",
    "    return child\n",
    "end\n",
    "\n",
    "# 突然変異\n",
    "function mutate!(brain::NeuroBrain, mutationRate::Float64)\n",
    "    dna = getDNA(brain)\n",
    "    \n",
    "    for i in eachindex(dna)\n",
    "        if rand() < mutationRate\n",
    "            dna[i] += randn() * 0.5\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    setDNA!(brain, dna)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神経進化でXOR問題を解く\n",
    "function neuroevolution_xor(;popSize=100, generations=200, mutationRate=0.1)\n",
    "    # XORの訓練データ\n",
    "    testCases = [\n",
    "        ([0.0, 0.0], 0.0),\n",
    "        ([0.0, 1.0], 1.0),\n",
    "        ([1.0, 0.0], 1.0),\n",
    "        ([1.0, 1.0], 0.0)\n",
    "    ]\n",
    "    \n",
    "    # 初期集団\n",
    "    population = [NeuroBrain(2, 4, 1) for _ in 1:popSize]\n",
    "    \n",
    "    bestFitness = Float64[]\n",
    "    \n",
    "    for gen in 1:generations\n",
    "        # 適応度計算\n",
    "        for brain in population\n",
    "            totalError = 0.0\n",
    "            for (inputs, target) in testCases\n",
    "                output = feedforward(brain.nn, inputs)[1]\n",
    "                totalError += (target - output)^2\n",
    "            end\n",
    "            brain.fitness = 1.0 / (1.0 + totalError)\n",
    "        end\n",
    "        \n",
    "        # 最良を記録\n",
    "        best = population[argmax([b.fitness for b in population])]\n",
    "        push!(bestFitness, best.fitness)\n",
    "        \n",
    "        if gen % 50 == 0\n",
    "            println(\"Gen $gen: Best fitness = $(round(best.fitness, digits=4))\")\n",
    "        end\n",
    "        \n",
    "        # 選択と交叉\n",
    "        matingPool = NeuroBrain[]\n",
    "        maxFit = maximum(b.fitness for b in population)\n",
    "        \n",
    "        for brain in population\n",
    "            n = Int(floor(brain.fitness / maxFit * 100))\n",
    "            for _ in 1:max(1, n)\n",
    "                push!(matingPool, brain)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # 次世代\n",
    "        newPopulation = NeuroBrain[]\n",
    "        \n",
    "        # エリート選択（最良を保存）\n",
    "        push!(newPopulation, best)\n",
    "        \n",
    "        for _ in 2:popSize\n",
    "            parentA = rand(matingPool)\n",
    "            parentB = rand(matingPool)\n",
    "            child = crossover(parentA, parentB)\n",
    "            mutate!(child, mutationRate)\n",
    "            push!(newPopulation, child)\n",
    "        end\n",
    "        \n",
    "        population = newPopulation\n",
    "    end\n",
    "    \n",
    "    # 最終結果\n",
    "    best = population[argmax([b.fitness for b in population])]\n",
    "    \n",
    "    println(\"\\nFinal Results:\")\n",
    "    for (inputs, target) in testCases\n",
    "        output = feedforward(best.nn, inputs)[1]\n",
    "        println(\"$(inputs) -> $(round(output, digits=3)) (expected: $target)\")\n",
    "    end\n",
    "    \n",
    "    return best, bestFitness\n",
    "end\n",
    "\n",
    "best_brain, neuro_fitness = neuroevolution_xor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線を比較\n",
    "plot(neuro_fitness, xlabel=\"Generation\", ylabel=\"Fitness\",\n",
    "     title=\"Neuroevolution XOR Learning\", legend=false, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 自律エージェントの脳として使用\n",
    "\n",
    "ニューラルネットワークを自律エージェントの意思決定に使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークで制御されるエージェント\n",
    "mutable struct SmartAgent\n",
    "    position::Vector{Float64}\n",
    "    velocity::Vector{Float64}\n",
    "    brain::NeuralNetwork\n",
    "    health::Float64\n",
    "end\n",
    "\n",
    "function SmartAgent(x::Float64, y::Float64)\n",
    "    # 入力: [食べ物までの角度, 距離, 現在の速度]\n",
    "    # 出力: [左に曲がる量, 右に曲がる量]\n",
    "    brain = NeuralNetwork(4, 8, 2, learningRate=0.0)\n",
    "    SmartAgent([x, y], [rand()*2-1, rand()*2-1], brain, 100.0)\n",
    "end\n",
    "\n",
    "function think!(agent::SmartAgent, food::Vector{Float64})\n",
    "    # 入力を計算\n",
    "    toFood = food - agent.position\n",
    "    dist = sqrt(sum(toFood.^2))\n",
    "    angle = atan(toFood[2], toFood[1])\n",
    "    heading = atan(agent.velocity[2], agent.velocity[1])\n",
    "    angleDiff = angle - heading\n",
    "    \n",
    "    inputs = [angleDiff / π, dist / 200, agent.velocity[1], agent.velocity[2]]\n",
    "    \n",
    "    # ニューラルネットワークで行動を決定\n",
    "    output = feedforward(agent.brain, inputs)\n",
    "    \n",
    "    # 出力を行動に変換（回転）\n",
    "    turnAmount = (output[1] - output[2]) * 0.1\n",
    "    \n",
    "    # 速度を回転\n",
    "    speed = sqrt(sum(agent.velocity.^2))\n",
    "    if speed > 0\n",
    "        heading += turnAmount\n",
    "        agent.velocity = [cos(heading), sin(heading)] * min(speed, 3.0)\n",
    "    end\n",
    "    \n",
    "    # 位置を更新\n",
    "    agent.position .+= agent.velocity\n",
    "    agent.health -= 0.5\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 パターン認識の例\n",
    "\n",
    "より実践的な例として、簡単なパターン認識タスクを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 円と四角形の分類\n",
    "function create_pattern_data(n::Int)\n",
    "    data = []\n",
    "    \n",
    "    for _ in 1:n\n",
    "        # 特徴: [縦横比, 角の数（近似）, 曲率（近似）]\n",
    "        if rand() < 0.5\n",
    "            # 円のような特徴\n",
    "            aspectRatio = 0.9 + rand() * 0.2  # ~1.0\n",
    "            corners = rand() * 0.2  # 少ない\n",
    "            curvature = 0.8 + rand() * 0.2  # 高い\n",
    "            label = [1.0, 0.0]  # 円\n",
    "        else\n",
    "            # 四角形のような特徴\n",
    "            aspectRatio = 0.5 + rand() * 1.0\n",
    "            corners = 0.7 + rand() * 0.3  # 多い（角がある）\n",
    "            curvature = rand() * 0.3  # 低い\n",
    "            label = [0.0, 1.0]  # 四角形\n",
    "        end\n",
    "        \n",
    "        push!(data, ([aspectRatio, corners, curvature], label))\n",
    "    end\n",
    "    \n",
    "    return data\n",
    "end\n",
    "\n",
    "# 訓練\n",
    "nn_pattern = NeuralNetwork(3, 6, 2, learningRate=0.3)\n",
    "trainingData = create_pattern_data(1000)\n",
    "\n",
    "errors = Float64[]\n",
    "for epoch in 1:100\n",
    "    shuffle!(trainingData)\n",
    "    totalError = 0.0\n",
    "    \n",
    "    for (inputs, targets) in trainingData\n",
    "        err = train!(nn_pattern, inputs, targets)\n",
    "        totalError += err\n",
    "    end\n",
    "    \n",
    "    push!(errors, totalError / length(trainingData))\n",
    "end\n",
    "\n",
    "# テスト\n",
    "testData = create_pattern_data(100)\n",
    "correct = 0\n",
    "\n",
    "for (inputs, targets) in testData\n",
    "    output = feedforward(nn_pattern, inputs)\n",
    "    predicted = argmax(output)\n",
    "    actual = argmax(targets)\n",
    "    if predicted == actual\n",
    "        correct += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Accuracy: $(correct / length(testData) * 100)%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(errors, xlabel=\"Epoch\", ylabel=\"Average Error\",\n",
    "     title=\"Pattern Classification Learning\", legend=false, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "### ニューラルネットワークの構成要素\n",
    "\n",
    "| 要素 | 説明 |\n",
    "|------|------|\n",
    "| ニューロン | 入力を処理して出力を生成する基本単位 |\n",
    "| 重み | 接続の強さ（学習で調整） |\n",
    "| バイアス | しきい値の調整 |\n",
    "| 活性化関数 | 非線形性を導入 |\n",
    "| 層 | ニューロンのグループ |\n",
    "\n",
    "### 学習アルゴリズム\n",
    "\n",
    "1. **パーセプトロン学習則**: 単層用、線形分離問題のみ\n",
    "2. **バックプロパゲーション**: 多層用、勾配降下法\n",
    "3. **神経進化**: 遺伝的アルゴリズムで重みを最適化\n",
    "\n",
    "### 重要な概念\n",
    "\n",
    "- **順伝播**: 入力から出力への計算\n",
    "- **誤差逆伝播**: 誤差を逆方向に伝播して重みを更新\n",
    "- **学習率**: 重み更新の大きさを制御\n",
    "- **過学習**: 訓練データに過度に適応すること\n",
    "\n",
    "### 応用分野\n",
    "\n",
    "- 画像認識\n",
    "- 自然言語処理\n",
    "- ゲームAI\n",
    "- 自律エージェントの制御\n",
    "- 予測・分類タスク"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "name": "julia",
   "version": "1.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
